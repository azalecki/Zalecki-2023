---
title: "Who is the public in public libraries? Exploring the spatial variations of the changing dynamics of library services in Chicago, IL"
author: "Alexandra Ola Zalecki"
date: "`r Sys.Date()`"
output: html_document
editor_options:
  markdown:
    wrap: sentence
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../docs/report") })
---

# Abstract

Rising inequality and increased privatization of space in urban landscapes is bringing attention to some of the only public spaces left: libraries. This study analyzes to what extent library service areas differ along lines of inequality like race, class, etc. This study will delineate library catchment areas in Chicago, IL and compare them with socio-economic data at the tract and block level. This analysis is the first part of a two pronged methods that aims to answer the question of to what extent the catchment areas are distinct. 

## Study Metadata

- `Key words`: public space, libraries, population weighted aggregation, service areas 
- `Subject`: Social and Behavioral Sciences: Geography: Human Geography
- `Date created`: 11/28/2023
- `Date modified`: 1/28/2024
- `Spatial Coverage`: Chicago, IL
- `Spatial Resolution`: Census Tracts, Census Blocks, Library Service Areas
- `Spatial Reference System`: EPSG:32616 
- `Temporal Coverage`: 2017-Present
- `Temporal Resolution`: Specify the temporal resolution of your study---i.e. the duration of time for which each observation represents or the revisit period for repeated observations

# Study design

This study is a reproduction of my own **an original study**. As part of my independent research work with Professor Peter Nelson, I created a workflow in QGIS to answer the question: How do library service catchment areas differ along lines of race, class, gender, etc? In order to streamline this research and make it reproducible/replicable I decided to reproduce the workflow in R and create a research compendium for it as part of my final independent project in GEOG0361: Open GIScience.

This research aims to answer the following two questions. How do library service catchment areas differ along lines of race, class, gender, etc. How do the public services in these catchment areas reflect the nature of their local constituents? 

# Materials and procedure

## Computational environment

```{r environment-setup, warning = FALSE}
# record all the packages you are using here
# this includes any calls to library(), require(),
# and double colons such as here::i_am()
packages <- c( 
  "tidycensus", "tidyverse", "sf", "classInt", "readr", "tigris",
  "rgdal","rstudioapi", "here", "s2", "pastecs", "tmap", "knitr", 
  "kableExtra", "broom", "leaflet", "usethis", "deldir", "spatstat", "webshot"
)

# force all conflicts to become errors
# if you load dplyr and use filter(), R has to guess whether you mean dplyr::filter() or stats::filter()
# the conflicted package forces you to be explicit about this
# disable at your own peril
# https://conflicted.r-lib.org/
require(conflicted)

# load and install required packages
# https://groundhogr.com/
if (!require(groundhog)) {
  install.packages("groundhog")
  require(groundhog)
}

if(!require(here)){
  install.packages("here")
  require(here)
}

# this date will be used to determine the versions of R and your packages
# it is best practice to keep R and its packages up to date
groundhog.day <- "2023-06-26"
set.groundhog.folder("../../data/scratch/groundhog/")

# this replaces any library() or require() calls
groundhog.library(packages, groundhog.day)
# you may need to install a correct version of R
# you may need to respond OK in the console to permit groundhog to install packages
# you may need to restart R and rerun this code to load installed packages
# In RStudio, restart r with Session -> Restart Session

# record the R processing environment
# alternatively, use devtools::session_info() for better results
writeLines(
  capture.output(sessionInfo()),
  here("procedure", "environment", paste0("r-environment-", Sys.Date(), ".txt"))
)

# save package citations
knitr::write_bib(c(packages, "base"), file = here("software.bib"))

# set up default knitr parameters
# https://yihui.org/knitr/options/
knitr::opts_chunk$set(
  echo = FALSE, # Run code, show outputs (don't show code)
  fig.retina = 4,
  fig.width = 8,
  fig.path = paste0(here("results", "figures"), "/")
)

#Switch the graphics output from raster to vector
knitr::opts_chunk$set(dev="png")

```
## Data and variables

Each of the next subsections describes one data source.
Primary data sources are to include the following. 

```{r load-website-data}
# Load website survey data from the Github

web = read_csv("https://raw.githubusercontent.com/azalecki/Zalecki-2023/main/data/raw/public/WS_1_29_24.csv")
```

Secondary data sources for the study are to include the following. 

### Chicago Shapefile 

```{r download-places, eval= FALSE}
# Load in all places defined by the US Census 
il_places <- places(state = "IL")

il_places

saveRDS(il_places, here("data", "raw", "public", "il_places.RDS"))

```

```{r load-clip-places}

# Load Census places for Illinois
il_places <- readRDS(here("data", "raw", "public", "il_places.RDS"))

# Filter out Chicago from Census Places

chi <- il_places %>%
  dplyr::filter(NAME == 'Chicago')%>%
  st_set_crs(st_crs("+proj=utm +zone=16 +datum=WGS84"))

# Plot geometry

plot(chi$geometry)

```

```{r download-tracts-shp, eval= FALSE}

cook_tracts <- tracts(
                state = "IL",
                county = "Cook",
                cb = FALSE,
                resolution = "500k",
                year = 2020)%>%
  st_set_crs(st_crs("+proj=utm +zone=16 +datum=WGS84"))

saveRDS(cook_tracts, here("data", "raw", "public", "cook_tracts.RDS"))

```

```{r load-tracts-shp}

#Load in tracts for Chicago 

cook_tracts <- readRDS(here("data", "raw", "public", "cook_tracts.RDS"))

```
### American Community Survey(ACS) Demographic Data 

In order to simplify the workflow for this independent project I will be working with one variable, household, income that is derived from ACS data table B19001. Household income data is referenced at the Census tract level for the whole of Cook County. The year derived is 2021. 

I will add a more comprehensive list of variables as my senior research project progresses.

```{r api-key, install = TRUE}
# Load in Census API Key. To get your own key visit this website: 

census_api_key("058bab25964a0d33dc97ba789df8df55ba443855")
```

```{r query-acs-data, eval = FALSE}

# Query Social & Demographic data tables with Census tract boundaries

 # Age and Sex Table 

  as_acs <- get_acs(
    geography = "tract",
    table = "B01001",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()

 # Race & Ethnicity Table
 
  race_acs <- get_acs(
    geography = "tract",
    table = "B03002",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()
  
  # Household Type 
  
  hhold_acs <- get_acs(
    geography = "tract",
    table = "B11003",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()
  
  # Educational Attainment Table 

  edu_acs <- get_acs(
    geography = "tract",
    table = "B15003",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()
  
  # Disability Status Table 
  
  dis_acs <- get_acs(
    geography = "tract",
    table = "B18101",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()

  # School Enrollment 
 
  enr_acs <- get_acs(
    geography = "tract",
    table = "B14001",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()
 
# Employment Table 

  emp_acs <- get_acs(
    geography = "tract",
    table = "B23025",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()
 
# Nativity Table 
 
  nat_acs <- get_acs(
    geography = "tract",
    table = "B05012",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
    st_drop_geometry()
 
# Language Spoken at Home

   lang_acs <- get_acs(
     geography = "tract",
     table = "B06007",
     county = "Cook",
     state = "IL",
     year = 2016,
     output = "wide",
     cache_table = TRUE,
     geometry = TRUE,
     keep_geo_vars = TRUE)%>%
     st_drop_geometry()
 
# Computers at Home
 
  comp_acs <- get_acs(
    geography = "tract",
    table = "B28010",
    county = "Cook",
    state = "IL",
    year = 2021,
    output = "wide",
    cache_table = TRUE,
    geometry = TRUE,
    keep_geo_vars = TRUE)%>%
     st_drop_geometry()
 
 # Household Income Table 
inc_acs <- get_acs(
   geography = "tract",
   table = "B19001",
   county = "Cook",
   state = "IL",
   year = 2021,
   output = "wide",
   cache_table = TRUE,
   geometry = TRUE,
   keep_geo_vars = TRUE)%>%
    st_drop_geometry()

m_inc_acs <- get_acs(
   geography = "tract",
   table = "B19013",
   county = "Cook",
   state = "IL",
   year = 2021,
   output = "wide",
   cache_table = TRUE,
   geometry = TRUE,
   keep_geo_vars = TRUE) %>%
  st_set_crs(st_crs("+proj=utm +zone=16 +datum=WGS84"))

# Save query results

saveRDS(edu_acs, here("data", "raw", "public", "edu_acs.RDS"))
saveRDS(as_acs, here("data", "raw", "public", "as_acs.RDS"))
saveRDS(race_acs, here("data", "raw", "public", "race_acs.RDS"))
saveRDS(hhold_acs, here("data", "raw", "public", "hhold_acs.RDS"))
saveRDS(dis_acs, here("data", "raw", "public", "dis_acs.RDS"))
saveRDS(enr_acs, here("data", "raw", "public", "enr_acs.RDS"))
saveRDS(emp_acs, here("data", "raw", "public", "emp_acs.RDS"))
saveRDS(nat_acs, here("data", "raw", "public", "nat_acs.RDS"))
saveRDS(lang_acs, here("data", "raw", "public", "lang_acs.RDS"))
saveRDS(comp_acs, here("data", "raw", "public", "comp_acs.RDS"))
saveRDS(inc_acs, here("data", "raw", "public", "inc_acs.RDS"))
saveRDS(m_inc_acs, here("data", "raw", "public", "m_inc_acs.RDS"))

```

```{r load-queried-acs-data}

#Load ACS Data 

edu_acs <- readRDS(here("data", "raw", "public", "edu_acs.RDS"))
as_acs <- readRDS(here("data", "raw", "public", "as_acs.RDS"))
race_acs <- readRDS(here("data", "raw", "public", "race_acs.RDS"))
hhold_acs <- readRDS(here("data", "raw", "public", "hhold_acs.RDS"))
dis_acs <- readRDS(here("data", "raw", "public", "dis_acs.RDS"))
enr_acs <- readRDS(here("data", "raw", "public", "enr_acs.RDS"))
emp_acs <- readRDS(here("data", "raw", "public", "emp_acs.RDS"))
nat_acs <- readRDS(here("data", "raw", "public", "nat_acs.RDS"))
lang_acs <- readRDS(here("data", "raw", "public", "lang_acs.RDS"))
comp_acs <- readRDS(here("data", "raw", "public", "comp_acs.RDS"))
inc_acs <- readRDS(here("data", "raw", "public", "inc_acs.RDS"))
m_inc_acs <- readRDS(here("data", "raw", "public", "m_inc_acs.RDS"))

```

### Public Library Locations

Data for Chicago Public Library locations comes in CSV format with coordinate data. Prior to uploading the CSV file into the Github site I used Microsoft Excel to manually separate the Longitude and Latitude values into two separate columns. No other data manipulation was done in Excel. 

```{r cpl-data, warning = FALSE}

#Load Chicago Public Library addresses from CSV file found in Folder: data/raw/public/CPL-Locations.csv

cpl_data = read_csv("https://raw.githubusercontent.com/azalecki/Zalecki-2023/main/data/raw/public/CPL-Locations.csv") %>% tibble()

# Create points layer using Longitude and Latitude columns and set projection to UTM Zone 16, EPSG: 32616

points <- cpl_data %>%
  st_as_sf(coords = c("Longitude", "Latitude")) %>%
  st_set_crs(st_crs("+proj=utm +zone=16 +datum=WGS84"))

# Check class and geometry type of data 

class(points)
st_geometry_type(points)

# Plot points on map with Chicago boundary 

tmap_mode("plot")
tm_shape(chi) + 
  tm_borders() +
tm_shape(points) +
tm_dots(size=0.1, col = "red")

# cpl_points_m <- leaflet() %>% 
#                    addProviderTiles(providers$Esri.WorldGrayCanvas) %>% 
#                    addPolygons(data = chi, color = "gray", weight = 1, fillColor= "white", fillOpacity = 0.2)%>%
#                    addCircles(data = points, weight = 3, opacity= 1, color = "orange") 
# cpl_points_m



```

### Population Data and Census Blocks for Cook County, IL   
Because, the ACS data tables do not come with population data I have to bring in population data separately. Population data for this study is referenced at the block level for the whole of Cook County, IL. The year that the data is derived from is 2020.  

```{r download-pop-blocks, eval= FALSE}

# Query block geographic data from the 2020 Census population

blocks <- get_decennial(
  geography = "block",
  table = "P1",
  county = "Cook",
  state = "IL",
  year = 2020,
  output = "wide",
  cache_table = TRUE,
  geometry = TRUE,
  keep_geo_vars = TRUE) %>%
  st_set_crs(st_crs("+proj=utm +zone=16 +datum=WGS84"))

saveRDS(blocks, here("data", "raw", "public", "blocks.RDS"))

```

```{r load-blocks}

#Load block data 

blocks <- readRDS(here("data", "raw", "public", "blocks.RDS"))
                  
```
## Prior observations  

**Chicago Shapefile**

- [ ] data is not available yet
- [ ] data is available, but only metadata has been observed
- [X] metadata and descriptive statistics have been observed
- [ ] metadata and a pilot test subset or sample of the full dataset have been observed
- [ ] the full dataset has been observed. Explain how authors have already manipulated / explored the data.

**American Community Survey(ACS) Demographic Data** 

- [ ] data is not available yet
- [ ] data is available, but only metadata has been observed
- [X] metadata and descriptive statistics have been observed
- [ ] metadata and a pilot test subset or sample of the full dataset have been observed
- [ ] the full dataset has been observed. Explain how authors have already manipulated / explored the data.

**Public Library Locations**

- [ ] data is not available yet
- [ ] data is available, but only metadata has been observed
- [X] metadata and descriptive statistics have been observed
- [ ] metadata and a pilot test subset or sample of the full dataset have been observed
- [ ] the full dataset has been observed. Explain how authors have already manipulated / explored the data.

**Population Data and Census Blocks for Cook County, IL**  

- [ ] data is not available yet
- [ ] data is available, but only metadata has been observed
- [X] metadata and descriptive statistics have been observed
- [ ] metadata and a pilot test subset or sample of the full dataset have been observed
- [ ] the full dataset has been observed. Explain how authors have already manipulated / explored the data.


## Bias and threats to validity

**Edge/shape effects when creating polygons to represent library service/catchment areas**

Visualizing catchment areas for libraries is my first objective because, unlike primary schools that have definite attendance boundaries, libraries do not have proper "service areas." In the past, Thiessen/Voronoi polygons have been used to map catchment or service areas by proximity to points. As explained by Flitter et al(nd), GIS tools that generate Thiessen polygons draw shapes around a layer of point data where every location within one shape is nearer to its center point than all other points in the layer. These proximal regions assume that people are more likely to visit the library closest to them and as a result library services should reflect their local constituents. I recognize that this method has its flaws because this is not always the case. Some people may frequent libraries outside of their residential neighborhood for a variety of reasons and there is no way of accurately tracking that. The other option would be to draw buffers around library points like in the method we saw in the Kang et al. (year) study or calculate a network analysis. Thiessen polygons are, however, the simpler and computationally less intense option to a full-on network analysis. Although they might seem arbitrary I have attempted to improve the validity by including a population-weighted aggregation to more accurately estimate the neighborhood characteristics of the library service areas.

## Data transformations

### Block transformations
### Filter Population Blocks

Because I will be doing a population weighted aggregation I have to prepare the population data to some extent. I started by filtering out the blocks with no population data. The table of blocks that have population data was then clipped by the Chicago boundary. Then I selected for the columns that I wanted to keep working with to simplify the data table. The columns I selected for include: TRACTCE20, BLOCKCE20, GEOID, ALAND20, AWATER20, HOUSING20, POP20, geometry. 

```{r filter-blocks, warning = FALSE}

# Filter out blocks with no population data 
# Clip blocks by Chicago Boundary and simplify table by selecting for columns that I will need 


chi_blocks <- st_intersection(blocks, chi)%>%
  st_collection_extract("POLYGON")

chi_pop <- chi_blocks %>%
             dplyr::filter(POP20 > 0)%>%
             select(TRACTCE20, 
                    BLOCKCE20, 
                    GEOID, 
                    ALAND20, 
                    AWATER20, 
                    HOUSING20, 
                    POP20, 
                    geometry)

class(chi_pop)
plot(chi_pop$geometry)
```

```{r map-blocks, eval = FALSE}

tmap_mode("plot")
tm_shape(chi_pop) +
  tm_polygons(col="POP20", style= "jenks", title="Population", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

```

### Library Catchment Areas

To create the catchment areas I will create Thiessen/Voronoi polygons. This was done by using the st_voronoi() function on the library points data. Finally, I clipped the voronoi diagram by the Chicago shapefile. 

```{r voronoi-polygons, warning = FALSE}

# Generate Thiessen/Voronoi polygons from library points

vorpoly <- st_union(points)%>%
  st_voronoi()%>%
  st_collection_extract("POLYGON")%>%
  st_sf %>%
  st_cast


#Plot polygons to make sure they loaded
plot(vorpoly)
#class(vorpoly)

# Rejoin attributes from library points data to voronoi polygons 
# Because I had to union the points prior to st_voronoi() function the library attributes were lost 
vorpoly2 <- st_join(vorpoly, points)

# Clip voronoi polygons by Chicago boundary 
vor_chi <- st_intersection(vorpoly2, chi)
    
# Plot polygons on a map with librayr points

vor_map <- tmap_mode("plot")
           tm_shape(vor_chi)+
              tm_borders(col="black" )+
           tm_shape(chi) + 
              tm_borders() +
           tm_shape(points) +
              tm_dots(size=0.05, col = "red")
           
vor_map

```

### ACS data transformations

The ACS classifies the data it collects in its own way but I wanted to reclassify it into different categories.

After creating the new classifications, I selected for the columns that I wanted to keep and work with. This included all of the necessary geographic identifiers(STATEFP, COUNTYFP, TRACTCE, GEOID, NAME.X, ALAND, AWATER, geometry) and the source fields I had created in the previous step. Finally, I clipped the table by the Chicago shapefile so to only include tracts that are within Chicago's city boundaries. 

```{r new-bins}

# Make new columns for Age categories 

as_acs$child <- c(as_acs$B01001_003E + 
                  as_acs$B01001_004E + 
                  as_acs$B01001_005E + 
                  as_acs$B01001_027E + 
                  as_acs$B01001_028E + 
                  as_acs$B01001_029E)

as_acs$teen <- c(as_acs$B01001_006E + 
                 as_acs$B01001_007E +
                 as_acs$B01001_030E +
                 as_acs$B01001_031E)

as_acs$yad <- c(as_acs$B01001_008E + 
                as_acs$B01001_009E + 
                as_acs$B01001_010E +
                as_acs$B01001_011E + 
                as_acs$B01001_032E + 
                as_acs$B01001_033E + 
                as_acs$B01001_034E + 
                as_acs$B01001_035E)

as_acs$mad <- c(as_acs$B01001_012E + 
                as_acs$B01001_013E +
                as_acs$B01001_014E + 
                as_acs$B01001_015E +
                as_acs$B01001_016E +
                as_acs$B01001_036E +
                as_acs$B01001_037E +
                as_acs$B01001_038E +
                as_acs$B01001_039E +
                as_acs$B01001_040E)

as_acs$sen <- c(as_acs$B01001_017E + 
                as_acs$B01001_018E + 
                as_acs$B01001_019E +
                as_acs$B01001_020E +
                as_acs$B01001_021E +
                as_acs$B01001_022E +
                as_acs$B01001_023E + 
                as_acs$B01001_024E +
                as_acs$B01001_025E +
                as_acs$B01001_041E +
                as_acs$B01001_042E +
                as_acs$B01001_043E +
                as_acs$B01001_044E +
                as_acs$B01001_045E +
                as_acs$B01001_046E +
                as_acs$B01001_047E +
                as_acs$B01001_048E +
                as_acs$B01001_049E)

as_acs$t_male <- c(as_acs$B01001_002E)
as_acs$t_fem <- c(as_acs$B01001_026E)

# Rename Race columns to make the names more intuitive 

race_acs$white <- c(race_acs$B03002_003E)
race_acs$black <- c(race_acs$B03002_004E)
race_acs$native <- c(race_acs$B03002_005E)
race_acs$asian <- c(race_acs$B03002_006E)
race_acs$hawpi <- c(race_acs$B03002_007E)
race_acs$other <- c(race_acs$B03002_008E)
race_acs$mixed <- c(race_acs$B03002_009E+ 
                      race_acs$B03002_010E + 
                      race_acs$B03002_011E)
race_acs$latinx <-c(race_acs$B03002_012E)

# Make new columns for Household Income 

inc_acs$hhi1 <- c(inc_acs$B19001_002E + 
                    inc_acs$B19001_003E + 
                    inc_acs$B19001_004E + 
                    inc_acs$B19001_005E)

inc_acs$hhi2 <- c(inc_acs$B19001_006E + 
                    inc_acs$B19001_007E + 
                    inc_acs$B19001_008E + 
                    inc_acs$B19001_009E + 
                    inc_acs$B19001_010E)

inc_acs$hhi3 <- c(inc_acs$B19001_011E +
                    inc_acs$B19001_012E)
inc_acs$hhi4 <- c(inc_acs$B19001_013E)

inc_acs$hhi5 <- c(inc_acs$B19001_014E + 
                    inc_acs$B19001_015E)

inc_acs$hhi6 <- c(inc_acs$B19001_016E)
inc_acs$hhi7 <- c(inc_acs$B19001_017E) 


# Make new columns for Employment status
# emp1 : civilian labor employed | emp0: civilian labor unemployed

 emp_acs$emp0 <- emp_acs$B23025_007E
 emp_acs$emp1 <- (emp_acs$B23025_002E)

# Rename columns for Nativity data. 
#Make new columns for Nativity status
#for0: native born | for1: foreign born

 nat_acs$for0 <- c(nat_acs$B05012_002E)
 nat_acs$for1 <- c(nat_acs$B05012_003E)

# Make new columns for Educational Attainment
# et1: no highschool diploma | et2: highschool diploma, equivalent, or some college  | et3: associates or bachelor's |et4: graduate or professional school

 edu_acs$et1 <- c(edu_acs$B15003_002E +
                    edu_acs$B15003_003E +
                    edu_acs$B15003_004E +
                    edu_acs$B15003_005E +
                    edu_acs$B15003_006E +
                    edu_acs$B15003_007E +
                    edu_acs$B15003_008E +
                    edu_acs$B15003_009E +
                    edu_acs$B15003_010E +
                    edu_acs$B15003_011E +
                    edu_acs$B15003_012E +
                    edu_acs$B15003_013E +
                    edu_acs$B15003_014E +
                    edu_acs$B15003_015E +
                    edu_acs$B15003_016E)

 edu_acs$et2 <- c(edu_acs$B15003_017E +
                    edu_acs$B15003_018E +
                    edu_acs$B15003_019E +
                    edu_acs$B15003_020E)

 edu_acs$et3 <- c(edu_acs$B15003_021E +
                    edu_acs$B15003_022E)

 edu_acs$et4 <- c(edu_acs$B15003_023E +
                    edu_acs$B15003_024E +
                    edu_acs$B15003_025E)
 

#Language spoken at home
lang_acs$lang1 <- lang_acs$B06007_002E
lang_acs$lang2 <- lang_acs$B06007_003E
lang_acs$lang3 <- lang_acs$B06007_006E

lang_acs$lang4 <- c(lang_acs$B06007_004E + 
                  lang_acs$B06007_007E)

lang_acs$lang5 <- c(lang_acs$B06007_005E + 
                  lang_acs$B06007_008E)

# Make new bins for School Enrollment by level of school
# se1: highschool or under | se2: undergraduate or graduate or professional school | se3: not enrolled in school

enr_acs$se1 <- c(enr_acs$B14001_003E +
                    enr_acs$B14001_004E +
                    enr_acs$B14001_005E +
                    enr_acs$B14001_006E +
                    enr_acs$B14001_006E)

enr_acs$se2 <- c(enr_acs$B14001_008E + enr_acs$B14001_009E)

enr_acs$se3 <- c(enr_acs$B14001_010E)
```

```{r filter-acs}

# filter acs data by the columns that you want to keep in the new aggregate table

hhi_tab <- inc_acs %>%
  dplyr::select(TRACTCE, hhi1, hhi2, hhi3, hhi4, hhi5, hhi6, hhi7)

emp_tab <- emp_acs %>% 
  dplyr::select(TRACTCE, emp0, emp1)

nat_tab <- nat_acs %>% 
  dplyr::select(TRACTCE, for0, for1)

edu_tab <- edu_acs %>% 
  dplyr::select(TRACTCE, et1, et2, et3, et4)

race_tab <- race_acs %>% 
  dplyr::select(TRACTCE, white, black, native, asian, hawpi, mixed, latinx)

m_inc_tab <- m_inc_acs %>%
  dplyr::select(TRACTCE, B19013_001E)

as_tab <- as_acs %>% 
  dplyr::select(TRACTCE, child, teen, yad, mad, sen)

lang_tab <-lang_acs %>%
  dplyr::select(TRACTCE, lang1, lang2, lang3, lang4, lang5)

enr_tab <- enr_acs %>%
  dplyr::select(TRACTCE, se1, se2, se3)

# Compile all of the ACS data that I transformed into one large table 
acs_table <- hhi_tab %>%
  inner_join(emp_tab)%>%
  inner_join(nat_tab)%>% 
  inner_join(edu_tab)%>% 
  inner_join(race_tab)%>%
  inner_join(m_inc_tab)%>% 
  inner_join(as_tab)%>%
  inner_join(lang_tab)%>%
  inner_join(enr_tab)

# %>%
#   st_as_sf()%>%
#   st_set_crs(st_crs("+proj=utm +zone=16 +datum=WGS84"))
# 
# fc_tracts_shp <- st_intersection(fc_tracts, chi) %>%
#    st_collection_extract("POLYGON")

# plot(fc_tracts_shp$geometry)

```

## Overlays and Join to Geometry 

```{r tracts-clip-chi}
# Use intersect overlay tool to clip all tracts in Cook County by Chicago's boundaries

chi_tracts <- st_intersection(cook_tracts, chi) %>%
   st_collection_extract("POLYGON")%>%
   left_join(acs_table, by= "TRACTCE")

class(chi_tracts)

plot(chi_tracts$geometry.x)

```

### Join Tract Level Data to Blocks  

My intentions for this part of the workflow were to generate a layer of centroids for the block data so I could join the population data to the tracts. I would do this two times. Once for the original tracts layer and a second time after I intersect the tracts with the voronoi diagram. I successfully generate a list of centroids for every block but when I join the centroids to the tracts layer, R makes a row for every centroid that was joined to a tract. This more than double the amount of observation in the data table. 

```{r centroids, warning = FALSE}

# Generate centroids for the blocks

cent_gen <- st_centroid(chi_pop)

centroids <- cent_gen %>%
  dplyr::select(TRACTCE20, BLOCKCE20, POP20)

# Map 
tmap_mode("plot")
tm_shape(centroids) +
    tm_dots(size=0.05, col = "black")


```

```{r centroids-to-tracts, warning = FALSE}

# Join block centroids to tracts to calculate population data for the tract

tr_cent <- st_join(chi_tracts, centroids)%>%
  dplyr::select(TRACTCE, POP20)%>%
   group_by(TRACTCE)%>%
   summarize(sumPOP = sum(POP20))


#join pop data to acs table

acs_centroids <-
 st_join(tr_cent, chi_tracts, by = "TRACTCE")%>%
  dplyr::filter(sumPOP > 0)

acs_tracts <- st_join( chi_tracts, tr_cent, by = "TRACTCE")%>%
  dplyr::filter(sumPOP > 0)

tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="sumPOP", title="Population Tract", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

```


```{r lib-id}

#Join library data to centroids and select for variables that I want to keep 

lib_ids <- st_join(centroids, vor_chi)%>%
              dplyr::select(TRACTCE20, BLOCKCE20, POP20, NAME)%>%
  st_drop_geometry()

```


```{r join-trct-to-blcks}
# Join soc variables to blocks with libids


f_blocks <- left_join(lib_ids, acs_centroids, by= c("TRACTCE20" = "TRACTCE.x"))%>%
  st_as_sf()

f_blocks <- f_blocks %>%
  mutate(prPOP = POP20/sumPOP,
                    pr_minc = B19013_001E * prPOP,
                    pr_black = black * prPOP,
                    pr_for1 = for1 * prPOP) %>%
   group_by(NAME.x) %>%
   summarize(sum_minc = sum(pr_minc),
             sum_black = sum(pr_black),
             sum_for = sum(pr_for1))%>%
            st_drop_geometry()


# after I group by and summarize i just need to join that data by a unique id back to the catchment area geometry so I can map with it

s_areas <- left_join(vor_chi, f_blocks, by= c("NAME" = "NAME.x"))%>%
            dplyr::select(NAME, Census.Tracts, sum_minc, sum_black, sum_for)
              
```

```{r map-s_areas}
tmap_mode("plot")
tm_shape(s_areas) +
  tm_polygons(col="sum_minc",style= "jenks", title="Med. Household Income", lwd= 0, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tmap_mode("plot")
tm_shape(s_areas) +
  tm_polygons(col="sum_black",style= "jenks", title="Black", lwd= 0, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tmap_mode("plot")
tm_shape(s_areas) +
  tm_polygons(col="sum_for",style= "jenks", title="Foreign", lwd= 0, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
```
### Join website data to soc-demo data
```{r}

```

### join website data to voronoi shapefile 
```{r}

web_vor_chi <- left_join(vor_chi, web, by="NAME")

tmap_mode("plot")
tm_shape(web_vor_chi) +
  tm_polygons(col="CIRC", style="jenks", title="Circulation", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="VIST", style="jenks", title="Visitation", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="COMPSES", style="jenks", title="Visitation", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="IP_HHELP", title="In Person Homework Help", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="HSPOT", title="HotSpot Lending", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="UMEDIA", title="YouMedia", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="CHBOOK", title="Chromebook Lending", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="MHSS", title="Mental Health and Social Services", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tm_shape(web_vor_chi) +
  tm_polygons(col="CITCORNER", title="Citizen Corner", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)


tm_shape(web_vor_chi) +
  tm_polygons(col="NE_MATER", title="Non English Language Materials", lwd= NA, palette = c("purple", "green"))+
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)



```

# Results

##Tract Maps 

###Median Income

```{r medinc-tract-map}

# Median Household Income

# Check out histogram
hist(acs_tracts$B19013_001E)

# Map
tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="B19013_001E", style="jenks", title="Med. Household Income", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)


```
###Racial Majority
```{r majrac-tract-map}
# Majority Race
acs_tracts <- mutate(acs_tracts, pctBlack= black/sumPOP )%>%
              mutate(acs_tracts, pctWhite= white/sumPOP)%>%
              mutate(acs_tracts, pctAsian= asian/sumPOP)%>%
              mutate(acs_tracts, pctMixed= mixed/sumPOP)%>%
              mutate(acs_tracts, pctLatinx= latinx/sumPOP)

# Case function for race categories
acs_tracts <- mutate(acs_tracts, majRace = case_when(
             pctWhite > .55 ~ 'White',
             pctBlack > .55 ~ 'Black',
             pctAsian > .55 ~ 'Asian',
             pctLatinx > .55 ~ 'LatinX',
             TRUE ~ 'Other'))

# Map 
tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="majRace", title="Racial Majority", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
```
###Nativity

```{r nat-tract-map}
# Nativity
acs_tracts <- mutate(acs_tracts, pctFor= for1/sumPOP )%>%
            mutate(acs_tracts, pctNat= for0/sumPOP)

# Check out histogram
hist(acs_tracts$for1)

# Map 

tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="for1", style="jenks", title="Total Foreign", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctFor", style="quantile", title="Pct Foreign", lwd= NA, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
```
###Employment
```{r emp-tract-maps}

acs_tracts <- mutate(acs_tracts, pctEmp= emp1/sumPOP)%>%
            mutate(acs_tracts, pctUnemp= emp0/sumPOP)

hist(acs_tracts$emp1)

tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="emp1",style="jenks", title="Total Employed", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctEmp",style="quantile", title="Pct Employed", lwd= NA, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)


# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctUnemp", title="Pct Unemployed per Capita", lwd= 0, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

```

###Educational attainment
```{r edu-att-tract-map}

#field calculate percents 

acs_tracts <- mutate(acs_tracts, pctET1= et1/sumPOP )%>%
            mutate(acs_tracts, pctET2= et2/sumPOP)%>%
            mutate(acs_tracts, pctET3= et3/sumPOP)%>%
            mutate(acs_tracts, pctET4= et4/sumPOP)

# Totals
tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="et1", style= "jenks", title="Total No High Dip", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="et2", style= "jenks", title="Total High Dip or Eq", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="et3", style= "jenks", title="Total Assoc. or Bach.", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="et4", style= "jenks", title="Total Grad or Prof", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

#Percents

# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctET1", style= "quantile", title="Pct No High Dip", lwd= NA, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
# 
# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctET2",style= "quantile", title="Pct High Dip or Eq", lwd= NA, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
# 
# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctET3",style= "quantile", title="Pct Assoc. or Bach.", lwd= NA, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)
# 
# tmap_mode("plot")
# tm_shape(acs_tracts) +
#   tm_polygons(col="pctET4",style= "quantile", title="Pct Grad or Prof", lwd= NA, border.col="lightgrey") +
# tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)


```

###Language spoken at home
```{r lang-map}

#Field calculate 
acs_tracts <- mutate(acs_tracts, pctEng= lang1/sumPOP )%>%
            mutate(acs_tracts, pctSpan= lang2/sumPOP)%>%
            mutate(acs_tracts, pctOther= lang3/sumPOP)%>%
            mutate(acs_tracts, pctWell= lang4/sumPOP)%>%
            mutate(acs_tracts, pctNWell= lang5/sumPOP)

#case
acs_tracts <- mutate(acs_tracts, majLang = case_when(
             pctWell > .60 ~ 'Very Well',
             pctNWell > .60 ~ 'Not Very Well',
             TRUE ~ 'Other'))

#map

tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="lang2", style="jenks", title="Total Spanish Speakers", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="lang3",style="jenks", title="Total Other Language", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

tmap_mode("plot")
tm_shape(acs_tracts) +
  tm_polygons(col="lang4",style="jenks", title="Total Speak English Not Very Well", lwd= NA, border.col="lightgrey") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = .8, asp=0.8)

```

##Descriptive Statistics for Web Survey

```{r m-sd-table-2}
# Table of means and standard deviation for explanatory variables

means <- web %>% summarise(
                  m_circ = mean(CIRC),
                  m_vist = mean(VIST), 
                  m_hspot = mean(HSPOT),
                  m_citcorn = mean(CITCORNER),
                  m_cybnav = mean(CYBNAV),
                  m_strooms = mean(STROOMS),
                  m_umedia = mean(UMEDIA),
                  m_neng = mean(NE_MATER),
                  m_comp = mean(COMP),
                  m_compses = mean(COMPSES),
                  m_mhss = mean(MHSS))%>%
                  tibble()%>%
                  t()

sds <- web %>% summarise(
                  sd_circ = sd(CIRC),
                  sd_vist = sd(VIST), 
                  sd_hspot = sd(HSPOT),
                  sd_citcorn = sd(CITCORNER),
                  sd_cybnav = sd(CYBNAV),
                  sd_strooms = sd(STROOMS),
                  sd_umedia = sd(UMEDIA),
                  sd_neng = sd(NE_MATER),
                  sd_comp = sd(COMP),
                  sd_compses = sd(COMPSES),
                  sd_mhss = sd(MHSS))%>%
                  tibble()%>%
                  t()
print(means)
print(sds)

```

```{r ws-histograms}

 hist(web$CIRC)
 hist(web$VIST)
 hist(web$COMPSES)
```

```{r exclude-outliers}

```

```{r scatter-test}

ggplot(web, aes(x = VIST, y = CIRC)) +
  geom_point(size = 2) +
  geom_text(aes(label = NAME), vjust = -1, size= 2)
```
# Discussion

Under construction. 

# Integrity Statement

The authors of this preregistration state that they completed this preregistration to the best of their knowledge and that no other preregistration exists pertaining to the same hypotheses and research.

This report is based upon the template for Reproducible and Replicable Research in Human-Environment and Geographical Sciences, DOI:[10.17605/OSF.IO/W29MQ](https://doi.org/10.17605/OSF.IO/W29MQ)

# References
Flitter, H., Weckenbrock, P., & Weibel, R. (n.d.). Thiessen Polygon. Retrieved December 16, 2023, from http://www.gitta.info/Accessibilit/en/html/UncProxAnaly_learningObject4.html
